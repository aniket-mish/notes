# notes


triton inference server

kubernetes

inference

onnx

vllm

metrics

capacity planning

instances


